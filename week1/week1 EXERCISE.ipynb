{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os \n",
    "import requests \n",
    "from openai import OpenAI \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks so good so far\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True) \n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key: \n",
    "    print(\"api key is not present, please check\")\n",
    "elif not api_key.startswith(\"sk-proj\"):\n",
    "    print(\"An API was found but doesn't start with sk-proj-, please check on that\")\n",
    "else:\n",
    "    print(\"looks so good so far\")\n",
    "\n",
    "\n",
    "openai = OpenAI(); \n",
    "\n",
    "\n",
    "def user_prompt_for(question): \n",
    "    user_prompt=f\"please take the following technical question, and provide the explaination in depth, ${question}\"\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Apache Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant, and scalable data streaming applications. To understand how Kafka works, we'll explore its architecture, core concepts, and some components. \n",
       "\n",
       "**1. Core Concepts:**\n",
       "\n",
       "   - **Broker**: A Kafka server that stores and serves data. A typical Kafka cluster consists of multiple brokers to balance load and provide fault tolerance.\n",
       "   \n",
       "   - **Topic**: A category or feed name to which records are published. Topics act as message queues.\n",
       "    \n",
       "   - **Partition**: Each topic is divided into partitions, which are ordered, immutable sequences of records. Each partition can be distributed among different brokers to allow parallel processing and increase scalability.\n",
       "\n",
       "   - **Producer**: An application that writes (publishes) data to Kafka topics.\n",
       "\n",
       "   - **Consumer**: An application that reads (subscribes to) data from Kafka topics.\n",
       "\n",
       "   - **Consumer Group**: A group of one or more consumers that work together to consume messages from a topic. Within a consumer group, each partition is consumed by only one consumer, allowing for load balancing and fault tolerance.\n",
       "\n",
       "   - **Offset**: A unique identifier for each record in a partition, which allows consumers to track which records have been read. Offsets are sequential within a partition.\n",
       "\n",
       "**2. Architecture**: \n",
       "\n",
       "Kafka is designed around a distributed architecture featuring:\n",
       "\n",
       "   - **Cluster**: A Kafka cluster consists of multiple brokers. Each broker is responsible for storing data for different partitions of the configured topics.\n",
       "\n",
       "   - **Replication**: For fault tolerance, partitions can have replicas distributed across different brokers. When a producer sends data to a topic, the data is written to the leader partition, and the replicas will have the same data ultimately. If a broker fails, a replica can take over as the leader.\n",
       "\n",
       "   - **Zookeeper** (in earlier versions): Kafka traditionally relied on Zookeeper for managing cluster metadata, leader election, and topic partition assignments. However, Kafka has been moving towards a more self-contained architecture without Zookeeper (as of Kafka 2.8.0).\n",
       "\n",
       "**3. Data Flow**:\n",
       "\n",
       "When using Kafka, the typical data flow is as follows:\n",
       "\n",
       "   - **1. Produce Data**: A producer sends data to a Kafka topic by publishing messages. The producer can choose to send data synchronously or asynchronously, depending on the configuration.\n",
       "\n",
       "   - **2. Storage in Partitions**: The data is appended to one of the topic's partitions, based on a partitioning strategy (e.g., round-robin, or based on a key).\n",
       "\n",
       "   - **3. Consume Data**: A consumer retrieves data from a Kafka topic. It can do so either by pulling data regularly or by using a push model, depending on the setup.\n",
       "\n",
       "   - **4. Offset Management**: The consumer keeps track of offsets to remember which records have been read. The consumer can commit offsets automatically or manually.\n",
       "\n",
       "**4. Scalability and Fault Tolerance**:\n",
       "\n",
       "   - Kafka is designed to handle large volumes of data with horizontal scalability, meaning you can add more brokers to handle more data and consumers.\n",
       "\n",
       "   - Fault tolerance is achieved through data replication; if one broker goes down, another broker with the replicated data can take over without data loss.\n",
       "\n",
       "**5. Performance**:\n",
       "\n",
       "Kafka can handle millions of messages per second, owing to its design decisions: use of write-ahead logs, efficient batch processing, and zero-copy technology that minimizes disk I/O.\n",
       "\n",
       "**6. Use Cases**:\n",
       "\n",
       "Common use cases for Kafka include:\n",
       "\n",
       "   - Real-time analytics and monitoring.\n",
       "   - Log aggregation from various services.\n",
       "   - Event sourcing.\n",
       "   - Stream processing with tools like Kafka Streams or Apache Flink.\n",
       "\n",
       "### Questions for Clarification:\n",
       "\n",
       "1. Are you looking for a specific aspect of Kafka, like deployment, configuration best practices, or specific use cases?\n",
       "   \n",
       "2. Do you have familiarity with any messaging system, or would it help to compare Kafka's architecture to a different one, such as RabbitMQ or MQTT?\n",
       "\n",
       "3. Would you be interested in learning more about how Kafka handles schema management or data serialization formats like Avro or Protobuf? \n",
       "\n",
       "Each of these areas can provide deeper insights into Kafka's capabilities!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "system_prompt = \"act as a PHD level technical expert in programming, and help in gently uncovering concepts by proactively asking clarifying questions and explaining the topics\"  \n",
    "stream = openai.chat.completions.create(\n",
    "    model = MODEL_GPT, \n",
    "    messages = [\n",
    "    {\"role\": \"system\",    \"content\": system_prompt},\n",
    "    {\"role\": \"user\",      \"content\": \"Please explain how kafka works in depth.\"}\n",
    "], \n",
    "    stream = True, \n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or ''\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
